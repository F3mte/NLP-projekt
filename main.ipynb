{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c9ae8a-8870-4412-b3e1-9722ae973083",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego, PWR 2024\n",
    "## Analiza sentymentu recenzji filmów\n",
    "Autorzy:\n",
    "- Dominik Ćwikowski 248914\n",
    "- Konrad Maciejczyk \n",
    "\n",
    "Repozytorium:  \n",
    "- https://github.com/F3mte/NLP-projekt\n",
    "\n",
    "Zbiór danych:  \n",
    "- https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98f3855-d25d-4aaa-a099-113ab447a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampleSubmission.csv', 'test.tsv', 'train.tsv']\n"
     ]
    }
   ],
   "source": [
    "# import basic labriaries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import pickle\n",
    "# disable warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# list files from input directory\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467030fa-2908-4a5e-bd60-8666dd8d9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK (The Natural Language Toolkit) is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90dff959-9e8e-4939-8f5a-2be0abbd558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# We all, of course, know what Keras is\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "import random\n",
    "from tensorflow.random import set_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, SimpleRNN \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam, RMSprop, SGD, Adadelta, Adagrad\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.config import list_physical_devices\n",
    "# set random seed for the session and also for tensorflow that runs in background for keras\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "# check available GPUs\n",
    "print(f\"Num GPUs Available: {len(list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200ffaed-a07e-4f1e-8585-e3584508c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and check training data\n",
    "train= pd.read_csv(\"./input/train.tsv\", sep=\"\\t\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b0a42c-9aea-4d4f-900d-da289932d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66292, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and check test data\n",
    "test = pd.read_csv(\"./input/test.tsv\", sep=\"\\t\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5940d1a-302c-4443-aa98-80cefd218599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning sentenes from data set\n",
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "    df.dropna(axis=0, inplace=True)                            # remove empty values\n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        review_text = BeautifulSoup(sent).get_text()           # remove html content\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)     # remove non-alphabetic characters\n",
    "        words = word_tokenize(review_text.lower())             # tokenize the sentences\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words] # lemmatize each word\n",
    "        reviews.append(lemma_words)                            # add to list that will be returned\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ece774-e6d2-4c14-86ef-767b9da042dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0afaf26-5299-451d-bb56-801c2a30eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                         | 1408/156060 [00:00<01:01, 2521.13it/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5136\\2429433190.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(sent).get_text()           # remove html content\n",
      "100%|███████████████████████████████████████████████████████████████████████| 156060/156060 [00:13<00:00, 11741.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 66291/66291 [00:05<00:00, 12510.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training sequence: 156060\n",
      "Length of testing sequence:  66291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaned reviews for both train and test set retrieved\n",
    "train_sentences = clean_sentences(train)\n",
    "test_sentences = clean_sentences(test)\n",
    "print(f\"Length of training sequence: {len(train_sentences)}\")\n",
    "print(f\"Length of testing sequence:  {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7b83c7-d63d-41b7-99a4-fe601afe4558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the dependent values and convert to one-hot encoded output using to_categorical\n",
    "y_target = to_categorical(train.Sentiment.values)\n",
    "num_classes = y_target.shape[1]\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260b3fc7-b8ae-42c7-9f70-9f977eded616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_sentences, y_target, test_size=0.1, stratify=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb1574c-4ff3-4180-957c-b49637cf3500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 140454/140454 [00:00<00:00, 1134990.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:         13745\n",
      "Length of the longest sentence: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create set of unique words and find the longest sentence length to determine the padding needed for other sentences to match it\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "for sent in tqdm(X_train):\n",
    "    unique_words.update(sent)\n",
    "    if(len_max<len(sent)):\n",
    "        len_max = len(sent)\n",
    "\n",
    "print(f\"Number of unique words:         {len(list(unique_words))}\")\n",
    "print(f\"Length of the longest sentence: {len_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779f349c-ab4e-45b4-a1e7-dab7ac3347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "#X_train = map(str, X_train)\n",
    "#X_val = map(str, X_val)\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0bceeca-0c39-4cbd-a82c-de3a03172f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass subsets through tokenizer\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5481083-1a0d-4ef9-aa79-cf82bbeabe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140454, 48) (15606, 48) (66291, 48)\n"
     ]
    }
   ],
   "source": [
    "# padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n",
    "# Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\n",
    "X_train = pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c7d0f7-44d1-4375-86f9-ed4eb9944b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that create model\n",
    "def create_model(architecture, optimizer, lr):\n",
    "    '''\n",
    "    architecture - choose among 4 possible model architectures:\n",
    "                   'LSTM-GRU', 'LSTM', 'GRU', 'RNN'\n",
    "    optimizer - choose among 5 possible optimizers:\n",
    "                'Adam', 'RMSprop', 'SGD', 'Momentum', 'Adadelta'\n",
    "    lr - choose learning rate for optimizer\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(list(unique_words)), 50, input_length=len_max)),\n",
    "    # choose model qrchitecture\n",
    "    if architecture == 'LSTM-GRU':\n",
    "        model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)),\n",
    "        model.add(GRU(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=False)),\n",
    "    elif architecture == 'LSTM':\n",
    "        model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)),\n",
    "        model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=False)),\n",
    "    elif architecture == 'GRU':\n",
    "        model.add(GRU(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)),\n",
    "        model.add(GRU(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=False)),\n",
    "    elif architecture == 'RNN':\n",
    "        model.add(SimpleRNN(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)),\n",
    "        model.add(SimpleRNN(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=False)),\n",
    "    # add last dense layers\n",
    "    model.add(Dense(len(list(unique_words)), activation='relu')),\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile model and print summary\n",
    "    if optimizer == 'Adam':\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    elif optimizer == 'RMSprop':\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=lr), metrics=['accuracy'])\n",
    "    elif optimizer == 'SGD':\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=lr), metrics=['accuracy'])\n",
    "    elif optimizer == 'Momentum':\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=lr, momentum=0.9), metrics=['accuracy'])\n",
    "    elif optimizer == 'Adadelta':\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adadelta(learning_rate=lr), metrics=['accuracy'])\n",
    "    elif optimizer == 'Adagrad':\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adagrad(learning_rate=lr), metrics=['accuracy'])\n",
    "    # print summary and return model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "944c8eff-e1a4-40aa-b83f-9f4212287c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting accuraccy\n",
    "def plot_accuracies(history):\n",
    "    acc=history.history['accuracy']\n",
    "    val_acc=history.history['val_accuracy']\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']\n",
    "\n",
    "    epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "    plt.plot(epochs, acc, 'r')\n",
    "    plt.plot(epochs, val_acc, 'b')\n",
    "    plt.title('Training and testing accuracy')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(['Training', 'testing'])\n",
    "    plt.show()\n",
    "    print(\"\")\n",
    "\n",
    "    plt.plot(epochs, loss, 'r')\n",
    "    plt.plot(epochs, val_loss, 'b')\n",
    "    plt.title('Training and testing loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['Training', 'testing'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fd22b8f-f6b6-472d-9b3f-b5bf9e35b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 48, 50)            687250    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 48, 64)            29440     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 13745)             893425    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13745)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 68730     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,703,805\n",
      "Trainable params: 1,703,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8779/8779 [==============================] - 352s 40ms/step - loss: 1.2271 - accuracy: 0.5122 - val_loss: 1.2085 - val_accuracy: 0.5135\n",
      "Epoch 2/10\n",
      "8779/8779 [==============================] - 356s 41ms/step - loss: 1.1992 - accuracy: 0.5166 - val_loss: 1.1697 - val_accuracy: 0.5237\n",
      "Epoch 3/10\n",
      "8779/8779 [==============================] - 366s 42ms/step - loss: 1.1473 - accuracy: 0.5368 - val_loss: 1.1010 - val_accuracy: 0.5561\n",
      "Epoch 4/10\n",
      "8779/8779 [==============================] - 365s 42ms/step - loss: 1.0799 - accuracy: 0.5693 - val_loss: 1.0223 - val_accuracy: 0.5988\n",
      "Epoch 5/10\n",
      "8779/8779 [==============================] - 373s 42ms/step - loss: 0.9887 - accuracy: 0.6050 - val_loss: 0.9328 - val_accuracy: 0.6267\n",
      "Epoch 6/10\n",
      "8779/8779 [==============================] - 369s 42ms/step - loss: 0.9163 - accuracy: 0.6299 - val_loss: 0.8861 - val_accuracy: 0.6391\n",
      "Epoch 7/10\n",
      "8779/8779 [==============================] - 366s 42ms/step - loss: 0.8734 - accuracy: 0.6446 - val_loss: 0.8561 - val_accuracy: 0.6491\n",
      "Epoch 8/10\n",
      "8779/8779 [==============================] - 364s 41ms/step - loss: 0.8471 - accuracy: 0.6515 - val_loss: 0.8427 - val_accuracy: 0.6576\n",
      "Epoch 9/10\n",
      "8779/8779 [==============================] - 368s 42ms/step - loss: 0.8280 - accuracy: 0.6577 - val_loss: 0.8333 - val_accuracy: 0.6604\n",
      "Epoch 10/10\n",
      "8779/8779 [==============================] - 366s 42ms/step - loss: 0.8154 - accuracy: 0.6624 - val_loss: 0.8279 - val_accuracy: 0.6623\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# save history\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperpara_tuning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_history.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_pi:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(history\u001b[38;5;241m.\u001b[39mhistory, file_pi)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# display results\u001b[39;00m\n\u001b[0;32m     21\u001b[0m plot_accuracies(history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# create inital model to perform learning rate tuning\n",
    "# Adagrad is used for now, later will be discarded for other optimizer\n",
    "model_name = 'learning_rate_0dot01'\n",
    "model = create_model(architecture='LSTM-GRU',\n",
    "                                    optimizer='Adagrad',\n",
    "                                    lr=0.01)\n",
    "# checkpoint callback to save best model during hyper-parameters tuning step\n",
    "checkpoint = ModelCheckpoint(f\"hyperpara_tuning\\\\{model_name}.keras\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "# fit model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=10, \n",
    "                    batch_size=16,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)\n",
    "# save history\n",
    "with open(f\"hyperpara_tuning\\\\{model_name}_history.pkl\", 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "# display results\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa4185-b0b8-4f11-9773-ca050428b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'learning_rate_0dot005'\n",
    "model = create_model(architecture='LSTM-GRU',\n",
    "                                    optimizer='Adagrad',\n",
    "                                    lr=0.005)\n",
    "# checkpoint callback to save best model during hyper-parameters tuning step\n",
    "checkpoint = ModelCheckpoint(f\"hyperpara_tuning\\\\{model_name}.keras\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "# fit model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=10, \n",
    "                    batch_size=16,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)\n",
    "# save history\n",
    "with open(f\"hyperpara_tuning\\\\{model_name}_history.pkl\", 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "# display results\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c6a48-6bbb-462a-8866-78eef15a8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'learning_rate_0dot0025'\n",
    "model = create_model(architecture='LSTM-GRU',\n",
    "                                    optimizer='Adagrad',\n",
    "                                    lr=0.0025)\n",
    "# checkpoint callback to save best model during hyper-parameters tuning step\n",
    "checkpoint = ModelCheckpoint(f\"hyperpara_tuning\\\\{model_name}.keras\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "# fit model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=10, \n",
    "                    batch_size=16,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)\n",
    "# save history\n",
    "with open(f\"hyperpara_tuning\\\\{model_name}_history.pkl\", 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "# display results\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b0c50-3152-403a-8942-f07243a6c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'learning_rate_0dot001'\n",
    "model = create_model(architecture='LSTM-GRU',\n",
    "                                    optimizer='Adagrad',\n",
    "                                    lr=0.001)\n",
    "# checkpoint callback to save best model during hyper-parameters tuning step\n",
    "checkpoint = ModelCheckpoint(f\"hyperpara_tuning\\\\{model_name}.keras\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "# fit model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=10, \n",
    "                    batch_size=16,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)\n",
    "# save history\n",
    "with open(f\"hyperpara_tuning\\\\{model_name}_history.pkl\", 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "# display results\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a38293-7c2e-44f6-b110-81ad10ec02ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
